{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJXfQxcyQvjHzwJ5Bje1Sg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Storm00212/Introduction-to-machine-learning-models/blob/main/Violence_detection_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mK26v4hkOwdd"
      },
      "outputs": [],
      "source": [
        "#1. System & File Handling\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import glob\n",
        "import shutil\n",
        "#2. Video Processing & Image Utilities\n",
        "import cv2                # OpenCV for reading videos & extracting frames\n",
        "import numpy as np\n",
        "from PIL import Image     # Optional for additional image manipulation\n",
        "#3. Data Analysis & Visualization\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Enable inline plotting for Colab\n",
        "%matplotlib inline\n",
        "#4. Machine Learning / Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#5. Deep Learning Layers and Tools\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# 6. Model Evaluation Tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import itertools\n",
        "#  7. Progress & Logging\n",
        "from tqdm import tqdm     # for progress bars\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Dataset Loading and Directory Setup\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Clone the public GitHub repository that contains the violence detection dataset\n",
        "!git clone https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos.git\n",
        "\n",
        "# Change the current working directory to the cloned repository\n",
        "os.chdir(\"A-Dataset-for-Automatic-Violence-Detection-in-Videos\")\n",
        "\n",
        "# Display the folder structure to confirm that the dataset is available\n",
        "for root, dirs, files in os.walk(\".\", topdown=True):\n",
        "    level = root.replace(os.getcwd(), '').count(os.sep)\n",
        "    indent = ' ' * 4 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "        print(f\"{subindent}{f}\")\n",
        "\n",
        "# Create organized folders for training and testing data\n",
        "base_dir = \"/content/violence_dataset\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# Define subdirectories for each class (violent and non-violent)\n",
        "train_violent_dir = os.path.join(train_dir, \"violent\")\n",
        "train_nonviolent_dir = os.path.join(train_dir, \"non_violent\")\n",
        "test_violent_dir = os.path.join(test_dir, \"violent\")\n",
        "test_nonviolent_dir = os.path.join(test_dir, \"non_violent\")\n",
        "\n",
        "# Create all directories if they do not already exist\n",
        "os.makedirs(train_violent_dir, exist_ok=True)\n",
        "os.makedirs(train_nonviolent_dir, exist_ok=True)\n",
        "os.makedirs(test_violent_dir, exist_ok=True)\n",
        "os.makedirs(test_nonviolent_dir, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdME_zZfUu8F",
        "outputId": "f65e6b0f-85d9-4f4d-8cf0-99854c6c205f",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'A-Dataset-for-Automatic-Violence-Detection-in-Videos'...\n",
            "remote: Enumerating objects: 376, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 376 (delta 3), reused 11 (delta 3), pack-reused 364 (from 1)\u001b[K\n",
            "Receiving objects: 100% (376/376), 1.02 GiB | 36.18 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (355/355), done.\n",
            "./\n",
            "    readme.md\n",
            "    .gitignore\n",
            "    violence-detection-dataset/\n",
            "        action-class-occurrences.csv\n",
            "        violent-action-classes.csv\n",
            "        nonviolent-action-classes.csv\n",
            "        non-violent/\n",
            "            cam2/\n",
            "                37.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                1.mp4\n",
            "                27.mp4\n",
            "                7.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                35.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                42.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                16.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                44.mp4\n",
            "                54.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                17.mp4\n",
            "                33.mp4\n",
            "                49.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "            cam1/\n",
            "                37.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                1.mp4\n",
            "                27.mp4\n",
            "                7.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                35.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                42.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                16.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                44.mp4\n",
            "                54.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                17.mp4\n",
            "                33.mp4\n",
            "                49.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "        violent/\n",
            "            cam2/\n",
            "                113.mp4\n",
            "                37.mp4\n",
            "                100.mp4\n",
            "                85.mp4\n",
            "                114.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                86.mp4\n",
            "                1.mp4\n",
            "                70.mp4\n",
            "                27.mp4\n",
            "                105.mp4\n",
            "                7.mp4\n",
            "                77.mp4\n",
            "                74.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                75.mp4\n",
            "                84.mp4\n",
            "                104.mp4\n",
            "                83.mp4\n",
            "                69.mp4\n",
            "                115.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                66.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                97.mp4\n",
            "                108.mp4\n",
            "                102.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                71.mp4\n",
            "                35.mp4\n",
            "                96.mp4\n",
            "                88.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                73.mp4\n",
            "                67.mp4\n",
            "                92.mp4\n",
            "                42.mp4\n",
            "                94.mp4\n",
            "                76.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                68.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                79.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                78.mp4\n",
            "                82.mp4\n",
            "                87.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                95.mp4\n",
            "                107.mp4\n",
            "                106.mp4\n",
            "                16.mp4\n",
            "                111.mp4\n",
            "                80.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                63.mp4\n",
            "                64.mp4\n",
            "                44.mp4\n",
            "                65.mp4\n",
            "                54.mp4\n",
            "                109.mp4\n",
            "                99.mp4\n",
            "                110.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                112.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                91.mp4\n",
            "                89.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                62.mp4\n",
            "                17.mp4\n",
            "                98.mp4\n",
            "                33.mp4\n",
            "                72.mp4\n",
            "                49.mp4\n",
            "                61.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                101.mp4\n",
            "                103.mp4\n",
            "                93.mp4\n",
            "                81.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "                90.mp4\n",
            "            cam1/\n",
            "                113.mp4\n",
            "                37.mp4\n",
            "                100.mp4\n",
            "                85.mp4\n",
            "                114.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                86.mp4\n",
            "                1.mp4\n",
            "                70.mp4\n",
            "                27.mp4\n",
            "                105.mp4\n",
            "                7.mp4\n",
            "                77.mp4\n",
            "                74.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                75.mp4\n",
            "                84.mp4\n",
            "                104.mp4\n",
            "                83.mp4\n",
            "                69.mp4\n",
            "                115.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                66.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                97.mp4\n",
            "                108.mp4\n",
            "                102.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                71.mp4\n",
            "                35.mp4\n",
            "                96.mp4\n",
            "                88.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                73.mp4\n",
            "                67.mp4\n",
            "                92.mp4\n",
            "                42.mp4\n",
            "                94.mp4\n",
            "                76.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                68.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                79.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                78.mp4\n",
            "                82.mp4\n",
            "                87.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                95.mp4\n",
            "                107.mp4\n",
            "                106.mp4\n",
            "                16.mp4\n",
            "                111.mp4\n",
            "                80.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                63.mp4\n",
            "                64.mp4\n",
            "                44.mp4\n",
            "                65.mp4\n",
            "                54.mp4\n",
            "                109.mp4\n",
            "                99.mp4\n",
            "                110.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                112.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                91.mp4\n",
            "                89.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                62.mp4\n",
            "                17.mp4\n",
            "                98.mp4\n",
            "                33.mp4\n",
            "                72.mp4\n",
            "                49.mp4\n",
            "                61.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                101.mp4\n",
            "                103.mp4\n",
            "                93.mp4\n",
            "                81.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "                90.mp4\n",
            "    .git/\n",
            "        packed-refs\n",
            "        index\n",
            "        HEAD\n",
            "        config\n",
            "        description\n",
            "        branches/\n",
            "        info/\n",
            "            exclude\n",
            "        hooks/\n",
            "            prepare-commit-msg.sample\n",
            "            post-update.sample\n",
            "            push-to-checkout.sample\n",
            "            pre-merge-commit.sample\n",
            "            commit-msg.sample\n",
            "            pre-applypatch.sample\n",
            "            fsmonitor-watchman.sample\n",
            "            pre-commit.sample\n",
            "            applypatch-msg.sample\n",
            "            update.sample\n",
            "            pre-rebase.sample\n",
            "            pre-receive.sample\n",
            "            pre-push.sample\n",
            "        logs/\n",
            "            HEAD\n",
            "            refs/\n",
            "                heads/\n",
            "                    master\n",
            "                remotes/\n",
            "                    origin/\n",
            "                        HEAD\n",
            "        refs/\n",
            "            heads/\n",
            "                master\n",
            "            tags/\n",
            "            remotes/\n",
            "                origin/\n",
            "                    HEAD\n",
            "        objects/\n",
            "            pack/\n",
            "                pack-22d483e08a84ebed7073b92081d86e37ba388a41.idx\n",
            "                pack-22d483e08a84ebed7073b92081d86e37ba388a41.pack\n",
            "            info/\n",
            "Directory structure ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mergin cam 1 and cam 2 videos\n",
        "def get_all_videos(folder_path):\n",
        "  videos=[]\n",
        "  cam1_path=os.path.join(folder_path, \"cam1\")\n",
        "  cam2_path=os.path.join(folder_path, \"cam2\")\n",
        "\n",
        "  if os.path.exists(cam1_path) and os.path.exists(cam2_path):\n",
        "    videos += [os.path.join(cam1_path, f) for f in os.listdir(cam1_path) if f.endswith(\".mp4\")] + [os.path.join(cam2_path, f) for f in os.listdir(cam2_path) if f.endswith(\".mp4\")]\n",
        "\n",
        "  return videos\n",
        "\n",
        "# Collect violent and non-violent videos\n",
        "dataset_path = os.path.join(os.getcwd(), \"violence-detection-dataset\")\n",
        "violent_videos = get_all_videos(os.path.join(dataset_path, \"violent\"))\n",
        "nonviolent_videos = get_all_videos(os.path.join(dataset_path, \"non-violent\"))\n",
        "\n",
        "print(f\"Total violent videos: {len(violent_videos)}\")\n",
        "print(f\"Total non-violent videos: {len(nonviolent_videos)}\")\n",
        "\n",
        "random.shuffle(violent_videos)\n",
        "random.shuffle(nonviolent_videos)\n",
        "\n",
        "def split_data(videos, split_ratio=0.8):\n",
        "  split_index =  int(len(videos) * split_ratio)\n",
        "  return videos[:split_index], videos[split_index:]\n",
        "\n",
        "\n",
        "violent_train, violent_test = split_data(violent_videos)\n",
        "nonviolent_train, nonviolent_test = split_data(nonviolent_videos)\n",
        "# Function to copy videos into the right directory\n",
        "def copy_files(file_list, target_folder):\n",
        "  for f in file_list:\n",
        "    shutil.copy(f, target_folder)\n",
        "# Copy all videos to the train/test folders\n",
        "copy_files(violent_train, train_violent_dir)\n",
        "copy_files(violent_test, test_violent_dir)\n",
        "copy_files(nonviolent_train, train_nonviolent_dir)\n",
        "copy_files(nonviolent_test, test_nonviolent_dir)\n",
        "\n",
        "print(\"Dataset organized successfully into training and testing folders.\")\n",
        "print(f\"Training set: {len(violent_train) + len(nonviolent_train)} videos\")\n",
        "print(f\"Testing set: {len(violent_test) + len(nonviolent_test)} videos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdE7FvuWihYE",
        "outputId": "1aee27a2-a54b-476f-c164-60c543ce69b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total violent videos: 230\n",
            "Total non-violent videos: 120\n",
            "Dataset organized successfully into training and testing folders.\n",
            "Training set: 280 videos\n",
            "Testing set: 70 videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frame extraction and processing\n",
        "def extract_frames(video_path, num_frames=20, resize=(64, 64)):\n",
        "  frames= []\n",
        "  try:\n",
        "    cap= cv2.VideoCapture(video_path)\n",
        "    total_frames= int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # Determining intervals to extract evenly spaced frames.\n",
        "    frame_interval= max(total_frames // num_frames, 1)\n",
        "    for i in range(0, total_frames, frame_interval):\n",
        "      ret, frame= cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame= cv2.resize(frame, resize)# Reesize each frame for uniformity.\n",
        "      # Normalise pixel values between 0 and 1\n",
        "      frame= frame/255.0\n",
        "      frames.append(frame)\n",
        "      # Stop once required frames are collected.\n",
        "      if len(frames) == num_frames:\n",
        "        break\n",
        "    cap.release()\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error extracting frames from {video_path}: {e}\")\n",
        "    return None\n",
        "\n",
        "data=[]\n",
        "labels=[]\n",
        "\n",
        "violent_path = \"/content/violence_dataset/train/violent\"\n",
        "nonviolent_path = \"/content/violence_dataset/train/non_violent\"\n",
        "\n",
        "# Extract frames from violent videos\n",
        "for folder in [\"\",]:\n",
        "    folder_path = violent_path\n",
        "    for video in tqdm(os.listdir(folder_path), desc=f\"Processing violent\"):\n",
        "        video_path = os.path.join(folder_path, video)\n",
        "        frames = extract_frames(video_path)\n",
        "        if frames is not None:\n",
        "            data.append(frames)\n",
        "            labels.append(1)  # Label 1 for violent\n",
        "\n",
        "# Extract frames from non-violent videos\n",
        "for folder in [\"\",]:\n",
        "    folder_path = nonviolent_path\n",
        "    for video in tqdm(os.listdir(folder_path), desc=f\"Processing non-violent\"):\n",
        "        video_path = os.path.join(folder_path, video)\n",
        "        frames = extract_frames(video_path)\n",
        "        if frames is not None:\n",
        "            data.append(frames)\n",
        "            labels.append(0)  # Label 0 for non-violent\n",
        "\n",
        "\n",
        "# Convert data and labels to NumPy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Frame extraction completed.\")\n",
        "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptMOle4SBRka",
        "outputId": "387f09db-d8e6-4480-dd3f-8826f2558bf0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing violent: 100%|██████████| 112/112 [00:35<00:00,  3.16it/s]\n",
            "Processing non-violent: 100%|██████████| 56/56 [00:16<00:00,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame extraction completed.\n",
            "Data shape: (168, 20, 64, 64, 3), Labels shape: (168,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SECTION 1: Imports\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "# SECTION 2: Data Splitting\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# SECTION 3: Data Augmentation\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "def augment_video(video):\n",
        "    return np.array([datagen.random_transform(frame) for frame in video])\n",
        "\n",
        "X_train_aug = np.array([augment_video(v) for v in X_train])\n",
        "y_train_aug = np.array(y_train)\n",
        "\n",
        "X_train_final = np.concatenate((X_train, X_train_aug))\n",
        "y_train_final = np.concatenate((y_train, y_train_aug))\n",
        "\n",
        "print(f\"Augmented training data: {X_train_final.shape}\")\n",
        "\n",
        "# SECTION 4: Model Definition (Fine-Tuned)\n",
        "def build_violence_detection_model(input_shape):\n",
        "    base_cnn = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "    for layer in base_cnn.layers[:-30]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_cnn.layers[-30:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.TimeDistributed(base_cnn, input_shape=input_shape),\n",
        "        layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
        "        layers.TimeDistributed(layers.BatchNormalization()),\n",
        "        layers.TimeDistributed(layers.MaxPooling2D((2, 2))),\n",
        "        layers.ConvLSTM2D(\n",
        "            64, (3, 3),\n",
        "            activation='relu',\n",
        "            return_sequences=False,\n",
        "            padding='same',\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3,\n",
        "            kernel_regularizer=regularizers.l2(1e-4)\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
        "        initial_learning_rate=1e-4,\n",
        "        first_decay_steps=5,\n",
        "        t_mul=2.0,\n",
        "        m_mul=0.8,\n",
        "        alpha=1e-6\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_violence_detection_model((20, 64, 64, 3))\n",
        "model.summary()\n",
        "\n",
        "# SECTION 5: Callbacks and Class Weights\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.3, min_lr=1e-6)\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_final),\n",
        "    y=y_train_final\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# SECTION 6: Model Training\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=60,\n",
        "    batch_size=8,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "# SECTION 7: Evaluation\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "model.save(\"violence_detection_optimized.keras\")\n",
        "\n",
        "# SECTION 8: Performance Plots\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "plt.title('Model Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='red')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
        "plt.title('Model Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "tFhYSr0kp1Mb",
        "outputId": "5ea136a7-fe7f-46f4-a9cc-58590a884cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (100, 20, 64, 64, 3), y_train: (100,)\n",
            "X_val: (34, 20, 64, 64, 3), y_val: (34,)\n",
            "X_test: (34, 20, 64, 64, 3), y_test: (34,)\n",
            "Augmented training data: (200, 20, 64, 64, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2048\u001b[0m) │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m1,179,712\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d (\u001b[38;5;33mConvLSTM2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,712</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,071,553\u001b[0m (95.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,071,553</span> (95.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,933,761\u001b[0m (60.78 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,933,761</span> (60.78 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,137,792\u001b[0m (34.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,137,792</span> (34.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: {0: np.float64(1.4705882352941178), 1: np.float64(0.7575757575757576)}\n",
            "Epoch 1/60\n",
            "\u001b[1m20/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:10\u001b[0m 14s/step - accuracy: 0.5588 - loss: 0.7055"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fc3e261"
      },
      "source": [
        "# Task\n",
        "Evaluate the model on the test set and optimize it to achieve high accuracy on testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15651da1"
      },
      "source": [
        "## Evaluate on test set\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the current model's performance on the test dataset to establish a baseline accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e690a2f1"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract frames from the test set videos and evaluate the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ad54898e",
        "outputId": "99161ea9-d8e2-4352-a04c-143bf9a965e4"
      },
      "source": [
        "# ============================\n",
        "# SECTION 1: Imports\n",
        "# ============================\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================\n",
        "# SECTION 2: Load Preprocessed Training Data\n",
        "# ============================\n",
        "# Each sample: (20 frames, 64, 64, 3)\n",
        "# frames.shape = (175, 20, 64, 64, 3)\n",
        "# labels.shape = (175,)\n",
        "\n",
        "# Normalize\n",
        "X = frames / 255.0\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# ============================\n",
        "# SECTION 3: Build Feature Extractor (ResNet50)\n",
        "# ============================\n",
        "base_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "base_resnet.trainable = False\n",
        "\n",
        "cnn_extractor = models.Sequential([\n",
        "    base_resnet,\n",
        "    layers.GlobalAveragePooling2D()\n",
        "])\n",
        "\n",
        "# ============================\n",
        "# SECTION 4: Extract Features per Frame\n",
        "# ============================\n",
        "def extract_features_for_videos(X):\n",
        "    n_samples, n_frames, h, w, c = X.shape\n",
        "    features = []\n",
        "    for i in tqdm(range(n_samples), desc=\"Extracting frame features\"):\n",
        "        video_feats = cnn_extractor.predict(X[i], verbose=0)\n",
        "        features.append(video_feats)\n",
        "    return np.array(features)\n",
        "\n",
        "X_train_feats = extract_features_for_videos(X_train)\n",
        "X_val_feats = extract_features_for_videos(X_val)\n",
        "\n",
        "print(\"Feature shape:\", X_train_feats.shape)  # (samples, 20, feature_dim)\n",
        "\n",
        "# ============================\n",
        "# SECTION 5: Build ConvLSTM Classifier\n",
        "# ============================\n",
        "def build_conv_lstm_classifier(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv1D(512, 3, padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Conv1D(256, 3, padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Bidirectional(layers.LSTM(256, return_sequences=True)),\n",
        "        layers.Bidirectional(layers.LSTM(128)),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_conv_lstm_classifier((X_train_feats.shape[1], X_train_feats.shape[2]))\n",
        "model.summary()\n",
        "\n",
        "# ============================\n",
        "# SECTION 6: Callbacks\n",
        "# ============================\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)\n",
        "\n",
        "# ============================\n",
        "# SECTION 7: Train\n",
        "# ============================\n",
        "history = model.fit(\n",
        "    X_train_feats, y_train,\n",
        "    validation_data=(X_val_feats, y_val),\n",
        "    epochs=40,\n",
        "    batch_size=8,\n",
        "    callbacks=[early_stop, lr_reduce],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# SECTION 8: Fine-Tune ResNet\n",
        "# ============================\n",
        "base_resnet.trainable = True\n",
        "for layer in base_resnet.layers[:int(0.7 * len(base_resnet.layers))]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Re-extract features with fine-tuned CNN\n",
        "X_train_feats_ft = extract_features_for_videos(X_train)\n",
        "X_val_feats_ft = extract_features_for_videos(X_val)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_history = model.fit(\n",
        "    X_train_feats_ft, y_train,\n",
        "    validation_data=(X_val_feats_ft, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=8,\n",
        "    callbacks=[early_stop, lr_reduce],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# SECTION 9: Plot Accuracy & Loss\n",
        "# ============================\n",
        "acc = history.history['accuracy'] + fine_tune_history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy'] + fine_tune_history.history['val_accuracy']\n",
        "loss = history.history['loss'] + fine_tune_history.history['loss']\n",
        "val_loss = history.history['val_loss'] + fine_tune_history.history['val_loss']\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, acc, 'b-', label='Train Acc')\n",
        "plt.plot(epochs, val_acc, 'r--', label='Val Acc')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, loss, 'b-', label='Train Loss')\n",
        "plt.plot(epochs, val_loss, 'r--', label='Val Loss')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================\n",
        "# SECTION 10: Evaluate on Test Set\n",
        "# ============================\n",
        "def extract_test_features(X_test):\n",
        "    n_samples, n_frames, h, w, c = X_test.shape\n",
        "    feats = []\n",
        "    for i in tqdm(range(n_samples), desc=\"Extracting test frame features\"):\n",
        "        feats.append(cnn_extractor.predict(X_test[i], verbose=0))\n",
        "    return np.array(feats)\n",
        "\n",
        "test_violent_path = \"/content/violence_dataset/test/violent\"\n",
        "test_nonviolent_path = \"/content/violence_dataset/test/non_violent\"\n",
        "\n",
        "test_violent_data, test_violent_labels = [], []\n",
        "for video in tqdm(os.listdir(test_violent_path), desc=\"Processing test violent\"):\n",
        "    video_path = os.path.join(test_violent_path, video)\n",
        "    frames = extract_frames(video_path)\n",
        "    if frames is not None:\n",
        "        test_violent_data.append(frames)\n",
        "        test_violent_labels.append(1)\n",
        "\n",
        "test_nonviolent_data, test_nonviolent_labels = [], []\n",
        "for video in tqdm(os.listdir(test_nonviolent_path), desc=\"Processing test non-violent\"):\n",
        "    video_path = os.path.join(test_nonviolent_path, video)\n",
        "    frames = extract_frames(video_path)\n",
        "    if frames is not None:\n",
        "        test_nonviolent_data.append(frames)\n",
        "        test_nonviolent_labels.append(0)\n",
        "\n",
        "X_test = np.concatenate((test_violent_data, test_nonviolent_data), axis=0)\n",
        "y_test = np.concatenate((test_violent_labels, test_nonviolent_labels), axis=0)\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "X_test_feats = extract_test_features(X_test)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_feats, y_test, verbose=0)\n",
        "print(f\"\\n✅ Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [20, 168]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-443311995.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# ============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2846\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [20, 168]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b624a3c"
      },
      "source": [
        "## Analyze results\n",
        "\n",
        "### Subtask:\n",
        "Analyze the evaluation metrics (accuracy, loss, potentially precision/recall) on the test set to understand the model's strengths and weaknesses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6feab8b"
      },
      "source": [
        "## Consider model improvements\n",
        "\n",
        "### Subtask:\n",
        "Based on the analysis, explore potential ways to improve the model, such as:\n",
        "    - **Hyperparameter Tuning**: Experiment with different learning rates, optimizers, batch sizes, etc.\n",
        "    - **Model Architecture Modifications**: Consider adding or removing layers, trying different base models, or incorporating temporal layers like LSTMs.\n",
        "    - **Data Augmentation**: Explore more advanced data augmentation techniques.\n",
        "    - **Regularization**: Adjust dropout rates or add other regularization methods if overfitting is observed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ae0806e"
      },
      "source": [
        "## Retrain and re-evaluate\n",
        "\n",
        "### Subtask:\n",
        "Retrain the model with the chosen improvements and re-evaluate on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14dcbd28"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the previous analysis and the identified areas for improvement, the next step is to retrain the model with potential hyperparameter tuning and architectural modifications. This involves defining a new model architecture, compiling it, training it, and then evaluating its performance on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cc70c8d",
        "outputId": "bf677514-0476-4fb8-ce9f-9c0c1f645144"
      },
      "source": [
        "# Consider a potentially improved model architecture.\n",
        "# This is an example modification, you might experiment further.\n",
        "# Adding more complex layers or trying a different pre-trained model could be options.\n",
        "def build_improved_model(input_shape):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False  # Start by freezing base layers\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5), # Increased dropout slightly\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4)), # Increased dense units\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4), # Increased dropout slightly\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define input shape based on flattened frames\n",
        "input_shape = (64, 64, 3)\n",
        "improved_model = build_improved_model(input_shape)\n",
        "\n",
        "# Compile the improved model with potentially tuned hyperparameters\n",
        "# Using a slightly different learning rate and optimizer as an example\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) # Lowered learning rate\n",
        "\n",
        "improved_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Retrain the improved model using the training data and augmentation\n",
        "# Use the same callbacks as before\n",
        "history_improved = improved_model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=8),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=40, # Keep potentially higher epochs to allow for learning\n",
        "    callbacks=[early_stop, lr_reduce],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fine-tune the improved model (optional, based on performance)\n",
        "# Unfreeze top layers for fine-tuning\n",
        "improved_base_model = improved_model.layers[0]\n",
        "improved_base_model.trainable = True\n",
        "for layer in improved_base_model.layers[:int(0.7 * len(improved_base_model.layers))]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate for fine-tuning\n",
        "improved_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), # Even lower learning rate\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_history_improved = improved_model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=8),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20, # Fine-tuning epochs\n",
        "    callbacks=[early_stop, lr_reduce],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the performance of the retrained model on the test set\n",
        "test_loss_improved, test_acc_improved = improved_model.evaluate(X_test_normalized, y_test, verbose=0)\n",
        "\n",
        "# Print the test loss and test accuracy of the retrained model\n",
        "print(f\"\\n✅ Retrained Model Test Accuracy: {test_acc_improved*100:.2f}%\")\n",
        "print(f\"Retrained Model Test Loss: {test_loss_improved:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 389ms/step - accuracy: 0.5784 - loss: 0.7737 - val_accuracy: 0.3235 - val_loss: 0.8498 - learning_rate: 5.0000e-05\n",
            "Epoch 2/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 235ms/step - accuracy: 0.6837 - loss: 0.7703 - val_accuracy: 0.3235 - val_loss: 0.8444 - learning_rate: 5.0000e-05\n",
            "Epoch 3/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196ms/step - accuracy: 0.6101 - loss: 0.7670 - val_accuracy: 0.3235 - val_loss: 0.8392 - learning_rate: 5.0000e-05\n",
            "Epoch 4/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 191ms/step - accuracy: 0.6418 - loss: 0.7625 - val_accuracy: 0.3235 - val_loss: 0.8438 - learning_rate: 5.0000e-05\n",
            "Epoch 5/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.6150 - loss: 0.7615 - val_accuracy: 0.3235 - val_loss: 0.8405 - learning_rate: 5.0000e-05\n",
            "Epoch 6/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 271ms/step - accuracy: 0.6339 - loss: 0.7587 - val_accuracy: 0.3235 - val_loss: 0.8363 - learning_rate: 5.0000e-05\n",
            "Epoch 7/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 195ms/step - accuracy: 0.6819 - loss: 0.7523 - val_accuracy: 0.3235 - val_loss: 0.8296 - learning_rate: 5.0000e-05\n",
            "Epoch 8/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 194ms/step - accuracy: 0.7033 - loss: 0.7489 - val_accuracy: 0.3235 - val_loss: 0.8220 - learning_rate: 5.0000e-05\n",
            "Epoch 9/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 271ms/step - accuracy: 0.6445 - loss: 0.7525 - val_accuracy: 0.3235 - val_loss: 0.8147 - learning_rate: 5.0000e-05\n",
            "Epoch 10/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - accuracy: 0.6784 - loss: 0.7430 - val_accuracy: 0.3235 - val_loss: 0.7997 - learning_rate: 5.0000e-05\n",
            "Epoch 11/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 190ms/step - accuracy: 0.6950 - loss: 0.7384 - val_accuracy: 0.3235 - val_loss: 0.8038 - learning_rate: 5.0000e-05\n",
            "Epoch 12/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - accuracy: 0.6237 - loss: 0.7451 - val_accuracy: 0.3235 - val_loss: 0.8009 - learning_rate: 5.0000e-05\n",
            "Epoch 13/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.7436 - loss: 0.7226\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 273ms/step - accuracy: 0.7391 - loss: 0.7233 - val_accuracy: 0.3235 - val_loss: 0.8002 - learning_rate: 5.0000e-05\n",
            "Epoch 14/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 196ms/step - accuracy: 0.6478 - loss: 0.7389 - val_accuracy: 0.3235 - val_loss: 0.8005 - learning_rate: 1.5000e-05\n",
            "Epoch 15/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 205ms/step - accuracy: 0.6962 - loss: 0.7316 - val_accuracy: 0.3235 - val_loss: 0.7985 - learning_rate: 1.5000e-05\n",
            "Epoch 16/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 264ms/step - accuracy: 0.6878 - loss: 0.7276 - val_accuracy: 0.3235 - val_loss: 0.7938 - learning_rate: 1.5000e-05\n",
            "Epoch 17/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 201ms/step - accuracy: 0.7078 - loss: 0.7286 - val_accuracy: 0.3235 - val_loss: 0.7917 - learning_rate: 1.5000e-05\n",
            "Epoch 18/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 195ms/step - accuracy: 0.6276 - loss: 0.7408 - val_accuracy: 0.3235 - val_loss: 0.7889 - learning_rate: 1.5000e-05\n",
            "Epoch 19/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 196ms/step - accuracy: 0.7069 - loss: 0.7252 - val_accuracy: 0.3235 - val_loss: 0.7857 - learning_rate: 1.5000e-05\n",
            "Epoch 20/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.6265 - loss: 0.7401 - val_accuracy: 0.3235 - val_loss: 0.7854 - learning_rate: 1.5000e-05\n",
            "Epoch 21/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 200ms/step - accuracy: 0.7033 - loss: 0.7254 - val_accuracy: 0.3235 - val_loss: 0.7864 - learning_rate: 1.5000e-05\n",
            "Epoch 22/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - accuracy: 0.6192 - loss: 0.7397 - val_accuracy: 0.3235 - val_loss: 0.7841 - learning_rate: 1.5000e-05\n",
            "Epoch 23/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.6637 - loss: 0.7306 - val_accuracy: 0.3235 - val_loss: 0.7843 - learning_rate: 1.5000e-05\n",
            "Epoch 24/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.6630 - loss: 0.7272 - val_accuracy: 0.3235 - val_loss: 0.7833 - learning_rate: 1.5000e-05\n",
            "Epoch 25/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - accuracy: 0.6338 - loss: 0.7377 - val_accuracy: 0.3235 - val_loss: 0.7809 - learning_rate: 1.5000e-05\n",
            "Epoch 26/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 246ms/step - accuracy: 0.6914 - loss: 0.7178 - val_accuracy: 0.3235 - val_loss: 0.7759 - learning_rate: 1.5000e-05\n",
            "Epoch 27/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 221ms/step - accuracy: 0.5962 - loss: 0.7457 - val_accuracy: 0.3235 - val_loss: 0.7727 - learning_rate: 1.5000e-05\n",
            "Epoch 28/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.6223 - loss: 0.7328 - val_accuracy: 0.3235 - val_loss: 0.7709 - learning_rate: 1.5000e-05\n",
            "Epoch 29/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - accuracy: 0.6401 - loss: 0.7327 - val_accuracy: 0.3235 - val_loss: 0.7680 - learning_rate: 1.5000e-05\n",
            "Epoch 30/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.6557 - loss: 0.7288 - val_accuracy: 0.3235 - val_loss: 0.7649 - learning_rate: 1.5000e-05\n",
            "Epoch 31/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - accuracy: 0.6366 - loss: 0.7397 - val_accuracy: 0.3235 - val_loss: 0.7634 - learning_rate: 1.5000e-05\n",
            "Epoch 32/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 193ms/step - accuracy: 0.6582 - loss: 0.7285 - val_accuracy: 0.4118 - val_loss: 0.7597 - learning_rate: 1.5000e-05\n",
            "Epoch 33/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 232ms/step - accuracy: 0.6982 - loss: 0.7138 - val_accuracy: 0.7353 - val_loss: 0.7555 - learning_rate: 1.5000e-05\n",
            "Epoch 34/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.6567 - loss: 0.7281 - val_accuracy: 0.6765 - val_loss: 0.7520 - learning_rate: 1.5000e-05\n",
            "Epoch 35/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - accuracy: 0.6097 - loss: 0.7411 - val_accuracy: 0.6765 - val_loss: 0.7465 - learning_rate: 1.5000e-05\n",
            "Epoch 36/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 196ms/step - accuracy: 0.6603 - loss: 0.7270 - val_accuracy: 0.6765 - val_loss: 0.7407 - learning_rate: 1.5000e-05\n",
            "Epoch 37/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.7018 - loss: 0.7134 - val_accuracy: 0.6765 - val_loss: 0.7355 - learning_rate: 1.5000e-05\n",
            "Epoch 38/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 207ms/step - accuracy: 0.6139 - loss: 0.7320 - val_accuracy: 0.6765 - val_loss: 0.7312 - learning_rate: 1.5000e-05\n",
            "Epoch 39/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - accuracy: 0.6643 - loss: 0.7162 - val_accuracy: 0.6765 - val_loss: 0.7272 - learning_rate: 1.5000e-05\n",
            "Epoch 40/40\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 248ms/step - accuracy: 0.6808 - loss: 0.7132 - val_accuracy: 0.6765 - val_loss: 0.7230 - learning_rate: 1.5000e-05\n",
            "Epoch 1/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.5556 - loss: 0.9780 - val_accuracy: 0.3235 - val_loss: 5.8914 - learning_rate: 1.0000e-06\n",
            "Epoch 2/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5202 - loss: 1.0630 - val_accuracy: 0.3235 - val_loss: 35.7929 - learning_rate: 1.0000e-06\n",
            "Epoch 3/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5920 - loss: 1.0617 - val_accuracy: 0.3235 - val_loss: 43.8931 - learning_rate: 1.0000e-06\n",
            "Epoch 4/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5370 - loss: 0.9953\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.999999992425728e-07.\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.5395 - loss: 0.9903 - val_accuracy: 0.3235 - val_loss: 78.2778 - learning_rate: 1.0000e-06\n",
            "Epoch 5/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.5513 - loss: 0.8919 - val_accuracy: 0.3235 - val_loss: 58.4974 - learning_rate: 3.0000e-07\n",
            "Epoch 6/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5100 - loss: 0.9589 - val_accuracy: 0.3235 - val_loss: 129.8741 - learning_rate: 3.0000e-07\n",
            "Epoch 7/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5595 - loss: 0.8350\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.000000318337698e-08.\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.5583 - loss: 0.8377 - val_accuracy: 0.3235 - val_loss: 278.7743 - learning_rate: 3.0000e-07\n",
            "Epoch 8/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4827 - loss: 0.9716 - val_accuracy: 0.3235 - val_loss: 388.1008 - learning_rate: 9.0000e-08\n",
            "\n",
            "✅ Retrained Model Test Accuracy: 31.75%\n",
            "Retrained Model Test Loss: 5.9433\n"
          ]
        }
      ]
    }
  ]
}