{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrSVwM0ffUNFQVCNyK+Hxt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Storm00212/Introduction-to-machine-learning-models/blob/main/Violence_detection_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK26v4hkOwdd"
      },
      "outputs": [],
      "source": [
        "#1. System & File Handling\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import glob\n",
        "import shutil\n",
        "#2. Video Processing & Image Utilities\n",
        "import cv2                # OpenCV for reading videos & extracting frames\n",
        "import numpy as np\n",
        "from PIL import Image     # Optional for additional image manipulation\n",
        "#3. Data Analysis & Visualization\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Enable inline plotting for Colab\n",
        "%matplotlib inline\n",
        "#4. Machine Learning / Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#5. Deep Learning Layers and Tools\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM, TimeDistributed, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# 6. Model Evaluation Tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import itertools\n",
        "#  7. Progress & Logging\n",
        "from tqdm import tqdm     # for progress bars\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Dataset Loading and Directory Setup\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Clone the public GitHub repository that contains the violence detection dataset\n",
        "!git clone https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos.git\n",
        "\n",
        "# Change the current working directory to the cloned repository\n",
        "os.chdir(\"A-Dataset-for-Automatic-Violence-Detection-in-Videos\")\n",
        "\n",
        "# Display the folder structure to confirm that the dataset is available\n",
        "for root, dirs, files in os.walk(\".\", topdown=True):\n",
        "    level = root.replace(os.getcwd(), '').count(os.sep)\n",
        "    indent = ' ' * 4 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "        print(f\"{subindent}{f}\")\n",
        "\n",
        "# Create organized folders for training and testing data\n",
        "base_dir = \"/content/violence_dataset\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# Define subdirectories for each class (violent and non-violent)\n",
        "train_violent_dir = os.path.join(train_dir, \"violent\")\n",
        "train_nonviolent_dir = os.path.join(train_dir, \"non_violent\")\n",
        "test_violent_dir = os.path.join(test_dir, \"violent\")\n",
        "test_nonviolent_dir = os.path.join(test_dir, \"non_violent\")\n",
        "\n",
        "# Create all directories if they do not already exist\n",
        "os.makedirs(train_violent_dir, exist_ok=True)\n",
        "os.makedirs(train_nonviolent_dir, exist_ok=True)\n",
        "os.makedirs(test_violent_dir, exist_ok=True)\n",
        "os.makedirs(test_nonviolent_dir, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdME_zZfUu8F",
        "outputId": "50d09141-ecc0-4131-bfbb-f438be13ed1e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'A-Dataset-for-Automatic-Violence-Detection-in-Videos'...\n",
            "remote: Enumerating objects: 376, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 376 (delta 3), reused 11 (delta 3), pack-reused 364 (from 1)\u001b[K\n",
            "Receiving objects: 100% (376/376), 1.02 GiB | 33.76 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (355/355), done.\n",
            "./\n",
            "    readme.md\n",
            "    .gitignore\n",
            "    violence-detection-dataset/\n",
            "        action-class-occurrences.csv\n",
            "        violent-action-classes.csv\n",
            "        nonviolent-action-classes.csv\n",
            "        non-violent/\n",
            "            cam2/\n",
            "                37.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                1.mp4\n",
            "                27.mp4\n",
            "                7.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                35.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                42.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                16.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                44.mp4\n",
            "                54.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                17.mp4\n",
            "                33.mp4\n",
            "                49.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "            cam1/\n",
            "                37.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                1.mp4\n",
            "                27.mp4\n",
            "                7.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                35.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                42.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                16.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                44.mp4\n",
            "                54.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                17.mp4\n",
            "                33.mp4\n",
            "                49.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "        violent/\n",
            "            cam2/\n",
            "                113.mp4\n",
            "                37.mp4\n",
            "                100.mp4\n",
            "                85.mp4\n",
            "                114.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                86.mp4\n",
            "                1.mp4\n",
            "                70.mp4\n",
            "                27.mp4\n",
            "                105.mp4\n",
            "                7.mp4\n",
            "                77.mp4\n",
            "                74.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                75.mp4\n",
            "                84.mp4\n",
            "                104.mp4\n",
            "                83.mp4\n",
            "                69.mp4\n",
            "                115.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                66.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                97.mp4\n",
            "                108.mp4\n",
            "                102.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                71.mp4\n",
            "                35.mp4\n",
            "                96.mp4\n",
            "                88.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                73.mp4\n",
            "                67.mp4\n",
            "                92.mp4\n",
            "                42.mp4\n",
            "                94.mp4\n",
            "                76.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                68.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                79.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                78.mp4\n",
            "                82.mp4\n",
            "                87.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                95.mp4\n",
            "                107.mp4\n",
            "                106.mp4\n",
            "                16.mp4\n",
            "                111.mp4\n",
            "                80.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                63.mp4\n",
            "                64.mp4\n",
            "                44.mp4\n",
            "                65.mp4\n",
            "                54.mp4\n",
            "                109.mp4\n",
            "                99.mp4\n",
            "                110.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                112.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                91.mp4\n",
            "                89.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                62.mp4\n",
            "                17.mp4\n",
            "                98.mp4\n",
            "                33.mp4\n",
            "                72.mp4\n",
            "                49.mp4\n",
            "                61.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                101.mp4\n",
            "                103.mp4\n",
            "                93.mp4\n",
            "                81.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "                90.mp4\n",
            "            cam1/\n",
            "                113.mp4\n",
            "                37.mp4\n",
            "                100.mp4\n",
            "                85.mp4\n",
            "                114.mp4\n",
            "                40.mp4\n",
            "                36.mp4\n",
            "                38.mp4\n",
            "                25.mp4\n",
            "                86.mp4\n",
            "                1.mp4\n",
            "                70.mp4\n",
            "                27.mp4\n",
            "                105.mp4\n",
            "                7.mp4\n",
            "                77.mp4\n",
            "                74.mp4\n",
            "                46.mp4\n",
            "                14.mp4\n",
            "                75.mp4\n",
            "                84.mp4\n",
            "                104.mp4\n",
            "                83.mp4\n",
            "                69.mp4\n",
            "                115.mp4\n",
            "                5.mp4\n",
            "                45.mp4\n",
            "                9.mp4\n",
            "                66.mp4\n",
            "                13.mp4\n",
            "                48.mp4\n",
            "                50.mp4\n",
            "                97.mp4\n",
            "                108.mp4\n",
            "                102.mp4\n",
            "                52.mp4\n",
            "                24.mp4\n",
            "                71.mp4\n",
            "                35.mp4\n",
            "                96.mp4\n",
            "                88.mp4\n",
            "                57.mp4\n",
            "                4.mp4\n",
            "                60.mp4\n",
            "                73.mp4\n",
            "                67.mp4\n",
            "                92.mp4\n",
            "                42.mp4\n",
            "                94.mp4\n",
            "                76.mp4\n",
            "                56.mp4\n",
            "                32.mp4\n",
            "                68.mp4\n",
            "                41.mp4\n",
            "                11.mp4\n",
            "                29.mp4\n",
            "                3.mp4\n",
            "                18.mp4\n",
            "                79.mp4\n",
            "                23.mp4\n",
            "                34.mp4\n",
            "                78.mp4\n",
            "                82.mp4\n",
            "                87.mp4\n",
            "                21.mp4\n",
            "                10.mp4\n",
            "                95.mp4\n",
            "                107.mp4\n",
            "                106.mp4\n",
            "                16.mp4\n",
            "                111.mp4\n",
            "                80.mp4\n",
            "                31.mp4\n",
            "                51.mp4\n",
            "                19.mp4\n",
            "                20.mp4\n",
            "                22.mp4\n",
            "                63.mp4\n",
            "                64.mp4\n",
            "                44.mp4\n",
            "                65.mp4\n",
            "                54.mp4\n",
            "                109.mp4\n",
            "                99.mp4\n",
            "                110.mp4\n",
            "                12.mp4\n",
            "                55.mp4\n",
            "                15.mp4\n",
            "                112.mp4\n",
            "                6.mp4\n",
            "                8.mp4\n",
            "                91.mp4\n",
            "                89.mp4\n",
            "                30.mp4\n",
            "                2.mp4\n",
            "                58.mp4\n",
            "                47.mp4\n",
            "                62.mp4\n",
            "                17.mp4\n",
            "                98.mp4\n",
            "                33.mp4\n",
            "                72.mp4\n",
            "                49.mp4\n",
            "                61.mp4\n",
            "                43.mp4\n",
            "                26.mp4\n",
            "                28.mp4\n",
            "                101.mp4\n",
            "                103.mp4\n",
            "                93.mp4\n",
            "                81.mp4\n",
            "                59.mp4\n",
            "                39.mp4\n",
            "                53.mp4\n",
            "                90.mp4\n",
            "    .git/\n",
            "        packed-refs\n",
            "        index\n",
            "        HEAD\n",
            "        config\n",
            "        description\n",
            "        branches/\n",
            "        info/\n",
            "            exclude\n",
            "        hooks/\n",
            "            prepare-commit-msg.sample\n",
            "            post-update.sample\n",
            "            push-to-checkout.sample\n",
            "            pre-merge-commit.sample\n",
            "            commit-msg.sample\n",
            "            pre-applypatch.sample\n",
            "            fsmonitor-watchman.sample\n",
            "            pre-commit.sample\n",
            "            applypatch-msg.sample\n",
            "            update.sample\n",
            "            pre-rebase.sample\n",
            "            pre-receive.sample\n",
            "            pre-push.sample\n",
            "        logs/\n",
            "            HEAD\n",
            "            refs/\n",
            "                heads/\n",
            "                    master\n",
            "                remotes/\n",
            "                    origin/\n",
            "                        HEAD\n",
            "        refs/\n",
            "            heads/\n",
            "                master\n",
            "            tags/\n",
            "            remotes/\n",
            "                origin/\n",
            "                    HEAD\n",
            "        objects/\n",
            "            pack/\n",
            "                pack-22d483e08a84ebed7073b92081d86e37ba388a41.idx\n",
            "                pack-22d483e08a84ebed7073b92081d86e37ba388a41.pack\n",
            "            info/\n",
            "Directory structure ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mergin cam 1 and cam 2 videos\n",
        "def get_all_videos(folder_path):\n",
        "  videos=[]\n",
        "  cam1_path=os.path.join(folder_path, \"cam1\")\n",
        "  cam2_path=os.path.join(folder_path, \"cam2\")\n",
        "\n",
        "  if os.path.exists(cam1_path) and os.path.exists(cam2_path):\n",
        "    videos += [os.path.join(cam1_path, f) for f in os.listdir(cam1_path) if f.endswith(\".mp4\")] + [os.path.join(cam2_path, f) for f in os.listdir(cam2_path) if f.endswith(\".mp4\")]\n",
        "\n",
        "  return videos\n",
        "\n",
        "# Collect violent and non-violent videos\n",
        "dataset_path = os.path.join(os.getcwd(), \"violence-detection-dataset\")\n",
        "violent_videos = get_all_videos(os.path.join(dataset_path, \"violent\"))\n",
        "nonviolent_videos = get_all_videos(os.path.join(dataset_path, \"non-violent\"))\n",
        "\n",
        "print(f\"Total violent videos: {len(violent_videos)}\")\n",
        "print(f\"Total non-violent videos: {len(nonviolent_videos)}\")\n",
        "\n",
        "random.shuffle(violent_videos)\n",
        "random.shuffle(nonviolent_videos)\n",
        "\n",
        "def split_data(videos, split_ratio=0.8):\n",
        "  split_index =  int(len(videos) * split_ratio)\n",
        "  return videos[:split_index], videos[split_index:]\n",
        "\n",
        "\n",
        "violent_train, violent_test = split_data(violent_videos)\n",
        "nonviolent_train, nonviolent_test = split_data(nonviolent_videos)\n",
        "# Function to copy videos into the right directory\n",
        "def copy_files(file_list, target_folder):\n",
        "  for f in file_list:\n",
        "    shutil.copy(f, target_folder)\n",
        "# Copy all videos to the train/test folders\n",
        "copy_files(violent_train, train_violent_dir)\n",
        "copy_files(violent_test, test_violent_dir)\n",
        "copy_files(nonviolent_train, train_nonviolent_dir)\n",
        "copy_files(nonviolent_test, test_nonviolent_dir)\n",
        "\n",
        "print(\"Dataset organized successfully into training and testing folders.\")\n",
        "print(f\"Training set: {len(violent_train) + len(nonviolent_train)} videos\")\n",
        "print(f\"Testing set: {len(violent_test) + len(nonviolent_test)} videos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdE7FvuWihYE",
        "outputId": "0d20539f-e43e-4d55-ee8f-eea2205e28f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total violent videos: 230\n",
            "Total non-violent videos: 120\n",
            "Dataset organized successfully into training and testing folders.\n",
            "Training set: 280 videos\n",
            "Testing set: 70 videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frame extraction and processing\n",
        "def extract_frames(video_path, num_frames=20, resize=(64, 64)):\n",
        "  frames= []\n",
        "  try:\n",
        "    cap= cv2.VideoCapture(video_path)\n",
        "    total_frames= int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # Determining intervals to extract evenly spaced frames.\n",
        "    frame_interval= max(total_frames // num_frames, 1)\n",
        "    for i in range(0, total_frames, frame_interval):\n",
        "      ret, frame= cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame= cv2.resize(frame, resize)# Reesize each frame for uniformity.\n",
        "      # Normalise pixel values between 0 and 1\n",
        "      frame= frame/255.0\n",
        "      frames.append(frame)\n",
        "      # Stop once required frames are collected.\n",
        "      if len(frames) == num_frames:\n",
        "        break\n",
        "    cap.release()\n",
        "\n",
        "    return np.array(frames)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error extracting frames from {video_path}: {e}\")\n",
        "    return None\n",
        "\n",
        "data=[]\n",
        "labels=[]\n",
        "\n",
        "violent_path = \"/content/violence_dataset/train/violent\"\n",
        "nonviolent_path = \"/content/violence_dataset/train/non_violent\"\n",
        "\n",
        "# Extract frames from violent videos\n",
        "for folder in [\"\",]:\n",
        "    folder_path = violent_path\n",
        "    for video in tqdm(os.listdir(folder_path), desc=f\"Processing violent\"):\n",
        "        video_path = os.path.join(folder_path, video)\n",
        "        frames = extract_frames(video_path)\n",
        "        if frames is not None:\n",
        "            data.append(frames)\n",
        "            labels.append(1)  # Label 1 for violent\n",
        "\n",
        "# Extract frames from non-violent videos\n",
        "for folder in [\"\",]:\n",
        "    folder_path = nonviolent_path\n",
        "    for video in tqdm(os.listdir(folder_path), desc=f\"Processing non-violent\"):\n",
        "        video_path = os.path.join(folder_path, video)\n",
        "        frames = extract_frames(video_path)\n",
        "        if frames is not None:\n",
        "            data.append(frames)\n",
        "            labels.append(0)  # Label 0 for non-violent\n",
        "\n",
        "\n",
        "# Convert data and labels to NumPy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Frame extraction completed.\")\n",
        "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptMOle4SBRka",
        "outputId": "8521a516-66cd-40b5-f3aa-6618d4aab74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing violent: 100%|██████████| 111/111 [00:36<00:00,  3.02it/s]\n",
            "Processing non-violent: 100%|██████████| 56/56 [00:17<00:00,  3.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame extraction completed.\n",
            "Data shape: (167, 20, 64, 64, 3), Labels shape: (167,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SECTION 1: Imports\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "# SECTION 2: Data Splitting\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# SECTION 3: Data Augmentation\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "def augment_video(video):\n",
        "    return np.array([datagen.random_transform(frame) for frame in video])\n",
        "\n",
        "X_train_aug = np.array([augment_video(v) for v in X_train])\n",
        "y_train_aug = np.array(y_train)\n",
        "\n",
        "X_train_final = np.concatenate((X_train, X_train_aug))\n",
        "y_train_final = np.concatenate((y_train, y_train_aug))\n",
        "\n",
        "print(f\"Augmented training data: {X_train_final.shape}\")\n",
        "\n",
        "# SECTION 4: Model Definition (Fine-Tuned)\n",
        "def build_violence_detection_model(input_shape):\n",
        "    base_cnn = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "    for layer in base_cnn.layers[:-30]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_cnn.layers[-30:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.TimeDistributed(base_cnn, input_shape=input_shape),\n",
        "        layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
        "        layers.TimeDistributed(layers.BatchNormalization()),\n",
        "        layers.TimeDistributed(layers.MaxPooling2D((2, 2))),\n",
        "        layers.ConvLSTM2D(\n",
        "            64, (3, 3),\n",
        "            activation='relu',\n",
        "            return_sequences=False,\n",
        "            padding='same',\n",
        "            dropout=0.3,\n",
        "            recurrent_dropout=0.3,\n",
        "            kernel_regularizer=regularizers.l2(1e-4)\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
        "        initial_learning_rate=1e-4,\n",
        "        first_decay_steps=5,\n",
        "        t_mul=2.0,\n",
        "        m_mul=0.8,\n",
        "        alpha=1e-6\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_violence_detection_model((20, 64, 64, 3))\n",
        "model.summary()\n",
        "\n",
        "# SECTION 5: Callbacks and Class Weights\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.3, min_lr=1e-6)\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_final),\n",
        "    y=y_train_final\n",
        ")\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# SECTION 6: Model Training\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=60,\n",
        "    batch_size=8,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "# SECTION 7: Evaluation\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "model.save(\"violence_detection_optimized.keras\")\n",
        "\n",
        "# SECTION 8: Performance Plots\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "plt.title('Model Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='red')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
        "plt.title('Model Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# adjust for accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tFhYSr0kp1Mb",
        "outputId": "6e1e36f3-baae-4732-c34b-5e4a02754f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (99, 20, 64, 64, 3), y_train: (99,)\n",
            "X_val: (34, 20, 64, 64, 3), y_val: (34,)\n",
            "X_test: (34, 20, 64, 64, 3), y_test: (34,)\n",
            "Augmented training data: (198, 20, 64, 64, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_8              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2048\u001b[0m) │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_9              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m1,179,712\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_10             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_11             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_2 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_8              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_9              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,712</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_10             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_11             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_lstm2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,071,553\u001b[0m (95.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,071,553</span> (95.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,933,761\u001b[0m (60.78 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,933,761</span> (60.78 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,137,792\u001b[0m (34.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,137,792</span> (34.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: {0: np.float64(1.5), 1: np.float64(0.75)}\n",
            "Epoch 1/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 16s/step - accuracy: 0.4465 - loss: 1.0522 - val_accuracy: 0.6176 - val_loss: 0.6939 - learning_rate: 3.2000e-05\n",
            "Epoch 2/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 14s/step - accuracy: 0.5413 - loss: 0.8189 - val_accuracy: 0.6765 - val_loss: 0.6637 - learning_rate: 3.5397e-05\n",
            "Epoch 3/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 14s/step - accuracy: 0.5125 - loss: 0.7889 - val_accuracy: 0.7059 - val_loss: 0.6494 - learning_rate: 4.0960e-05\n",
            "Epoch 4/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 14s/step - accuracy: 0.5419 - loss: 0.8946 - val_accuracy: 0.7059 - val_loss: 0.6649 - learning_rate: 3.1858e-05\n",
            "Epoch 5/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 14s/step - accuracy: 0.5150 - loss: 0.7600 - val_accuracy: 0.7059 - val_loss: 0.6743 - learning_rate: 1.2643e-05\n",
            "Epoch 6/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 14s/step - accuracy: 0.5283 - loss: 0.7620 - val_accuracy: 0.6176 - val_loss: 0.6638 - learning_rate: 3.9362e-07\n",
            "Epoch 7/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 14s/step - accuracy: 0.5525 - loss: 0.8617 - val_accuracy: 0.5882 - val_loss: 0.6699 - learning_rate: 3.1521e-05\n",
            "Epoch 8/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 13s/step - accuracy: 0.6214 - loss: 0.7729 - val_accuracy: 0.5882 - val_loss: 0.6420 - learning_rate: 2.6778e-05\n",
            "Epoch 9/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 14s/step - accuracy: 0.6282 - loss: 0.7684 - val_accuracy: 0.7059 - val_loss: 0.6504 - learning_rate: 1.9580e-05\n",
            "Epoch 10/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 13s/step - accuracy: 0.6155 - loss: 0.6399 - val_accuracy: 0.6765 - val_loss: 0.6130 - learning_rate: 1.1628e-05\n",
            "Epoch 11/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 14s/step - accuracy: 0.5028 - loss: 0.8267 - val_accuracy: 0.7059 - val_loss: 0.6099 - learning_rate: 4.7989e-06\n",
            "Epoch 12/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 13s/step - accuracy: 0.6141 - loss: 0.6923 - val_accuracy: 0.7353 - val_loss: 0.5892 - learning_rate: 7.0559e-07\n",
            "Epoch 13/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 14s/step - accuracy: 0.5408 - loss: 0.8454 - val_accuracy: 0.6765 - val_loss: 0.5767 - learning_rate: 2.6151e-05\n",
            "Epoch 14/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 13s/step - accuracy: 0.4896 - loss: 0.8291 - val_accuracy: 0.6176 - val_loss: 0.6508 - learning_rate: 2.5448e-05\n",
            "Epoch 15/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 14s/step - accuracy: 0.5604 - loss: 0.7192 - val_accuracy: 0.7353 - val_loss: 0.6255 - learning_rate: 2.4006e-05\n",
            "Epoch 16/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 14s/step - accuracy: 0.5488 - loss: 0.8018 - val_accuracy: 0.7353 - val_loss: 0.6346 - learning_rate: 2.1910e-05\n",
            "Epoch 17/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 13s/step - accuracy: 0.6113 - loss: 0.7340 - val_accuracy: 0.5882 - val_loss: 0.7595 - learning_rate: 1.9286e-05\n",
            "Epoch 18/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.6015 - loss: 0.6766 "
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4255634962.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# SECTION 6: Model Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mX_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mlearning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    715\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_schedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             ):\n\u001b[0;32m--> 717\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    718\u001b[0m                     \u001b[0;34m\"This optimizer was created with a `LearningRateSchedule`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     \u001b[0;34m\" object as its `learning_rate` constructor argument, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument."
          ]
        }
      ]
    }
  ]
}